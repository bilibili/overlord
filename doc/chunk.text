The node allocation algorithm aims to distribute chunks of nodes to achieve
the maximum of balance, aka. trying best to spread the failover of slaves
on the lost host most widely across the whole cluster.

Note:
  1. In this proof, we'll use "=" as assignment, and "==" as mathematical equal
     to conform to python's notations.
  2. It is hard to write mathematical symbols and notations in plain text,
     and therefore we adopt some of python's functions to help explanation.

Glossary:
1. chunk:
  A group of nodes in 4 across 2 hosts where 1 master node
  and 1 slave node each.
  Either HOST 1 or HOST 2 fails, redis node A and B
  should work fine after failing-over to the other host.
  -------------------------------
             one chunk
  | HOST   1 |      | HOST   2 |
  | master A | <--> | slave  A |
  | slave  B | <--> | master B |
  -------------------------------

2. link(ed) host:
  One chunk consists of 2 hosts, and we call them linked since the slave
  of the first host's master is on the second host, and vice versa.

Algorithm Description:
  Assume there're n hosts, whose corresponding available node number
  is host[i], where i in range(0, n) (aka. len(host) == n)

  The available node number on each host should meet the premise of:
  1. sum(host) % 4 == 0
  2. host[i] is integer for i in range(0, n)
  3. host[i] % 2 == 0 for i in range(0, n)
  4. host[m] <= sum(<host[i]>, i in range(0, n) && i != m),
     where m = max_index(host) (the index of the max value in host)

  Build up a link table (lt) which tracks the number of links
  from host[i] to host[j], where i, j in range(0, n):

  |        | HOST 0   | HOST 1   | HOST 2   | HOST 3   | HOST 4   |
  |--------|----------|--------- |----------|--------- |----------|
  | HOST 0 | 0        | lt[0, 1] | lt[0, 2] | lt[0, 3] | lt[0, 4] |
  | HOST 1 | lt[0, 1] | 0        | lt[1, 2] | lt[1, 3] | lt[1, 4] |
  | HOST 2 | lt[0, 2] | lt[1, 2] | 0        | lt[2, 3] | lt[2, 4] |
  | HOST 3 | lt[0, 3] | lt[1, 3] | lt[2, 3] | 0        | lt[3, 4] |
  | HOST 4 | lt[0, 4] | lt[1, 4] | lt[2, 4] | lt[3, 4] | 0        |
                  (A link table of hosts of 5)


  Steps:
  1. Check the input, if host does not meet up with the premise, reject.
  2. Initialize the lt (link table) to zeros.
  3. while any(host > 0) do following:
    3.1. m = max_index(host) (the index of the max value in host)
    3.2. llh = find_least_linked_host_index(m)
         (search the link table at row m, and find the minimum excluding self)

         i.e.
         |        | HOST 0   | HOST 1   | HOST 2   | HOST 3   | HOST 4   |
         |--------|----------|--------- |----------|--------- |----------|
         | HOST 0 | 0        | 2        | 4        | 4        | 4        |

         In such case, llh is 1 (HOST 1) (remember to exclude self, which is 0)
    3.3. establish a link between m and llh and create a chunk
    3.4  lt[m, llh] = lt[m, llh] + 1
    3.5. host[m]    = host[m]    - 2
    3.6. host[llh]  = host[llh]  - 2

Proof of convergence: (Prove that sum(host) == 0 finally)

  Lemma 1. For each iteration,
           host[m] <= sum(host[i], i in range(0, n) && i != m),
           where m = max_index(host)
           is always true.

  Proof of Lemma 1:
    According to premise 4, the initial state is true.

    Let host_next list be the next state of host after one iteration,
        p be the index of host linked with m in the new chunk.

    Therefore, we have:
      1. host_next[m] = host[m] - 2
      2. host_next[p] = host[p] - 2

    The next iteration state diverges into 2 following conditions:
    1) Assume m is still the max_index of host_next, which is:

               m == max_index(host_next)

       Left of inequation:

               host_next[m]
            == host[m] - 2

       Right of inequation:

               sum(host_next[i], i in range(0, n) && i != m)
            == sum(<host[i]>, i in range(0, n) && i != m) - 2

       Premise 4:

               host[m] <= sum(<host[i]>, i in range(0, n) && i != m)

       Therefore:

               host[m] - 2 <= sum(<host[i]>, i in range(0, n) && i != m) - 2

               host_next[m] <= sum(<host_next[i]>, i in range(0, n) && i != m)

    2) Assume q = max_index(host_next), and q != m


       Suppose q == p, then, host_next[q] == host_next[p] == host[p] - 2

       Since,
         1. host_next[m] == host[m] - 2
         2. host[m] > host[p] (Use condition 1 for host[m] == host[p])

       Then

         host_next[m] > host_next[p] == host_next[q]

       which is in contradiction to q being the max_index of host_next, and
       therefore

               q != p


       With q != m and q != p, we have
          1. host_next[q] == host[q] > host_next[m]
          2. host_next[m] == host[m] - 2
         ==> host[q] > host[m] - 2

       We also know that

               host[i] % 2 == 0 for i in range(0, n) ... Premise 3
               host[m] >= host[q]                    ... Premise 4

       We apply 3 conditions mentioned above together:
       1. host[i] % 2 == 0 for i in range(0, n)
       2. host[q] > host[m] - 2
       3. host[q] <= host[m]

       Then,
       1. host[m] - 2 < host[q] <= host[m]
       2. host[q] % 2 == 0

       Therefore,

               host[q] == host[m]

       Left of inequation:

               host_next[q]
            == host[q]

       Right of inequation:

               sum(<host_next[i]>, i in range(0, n) && i != q)
            == sum(<host[j]>) + host[m] - 2 - 2
            == sum(<host[j]>) + host[q] - 4 (since host[q] == host[m])
            , where j in range(0, n) && j != q && j != m

       Therefore, we need to prove that

               sum(<host[j]>) - 4 >= 0
            , where j in range(0, n) && j != q && j != m

        According to premise 2, 3, sum of host[j] results in
        3 following conditions:

        2.1) sum(<host[j]>) == 0

          Since
          1. q != m && q != p
          2. j != q && j != m
          3. host[p] >= 2 (host_next[p] = host[p] - 2 >= 0)

          Then,

                 sum(<host[j]>) >= host[p] >= 2

          which is in contradiction to sum(<host[j]>) == 0, and therefore
          condition 2.1 is logically impossible.

        2.2) sum(<host[j]>) == 2

          According to premise 3, non-zero values in host are
          {host[m], host[q], 2}

          Apply premise 1

               sum(host) % 4 ==0
           ==> host[m] + host[q] + 2 == 4*x
           ==> host[m] + host[q] == 4*x - 2
           ==> 2 * host[q] == 4*x - 2 (since host[q] == host[m])
           ==> host[q] == 2*x - 1

          host[q] == 2*x - 1 is in contradiction to premise 3, and therefore
          condition 2.2 is logically impossible.

        2.3) sum(<host[j]>) >= 4, conforms.

        Therefore,

               sum(<host[j]>) - 4 >= 0
           ==> host[q] <= host[q] + sum(<host[j]>) - 4
           ==> host[q] <= host[m] + sum(<host[j]>) - 4
           ==> host_next[q] <= host_next[m] + sum(<host_next[j]>)
           ==> host_next[q] <= sum(<host_next[i]>)
           , where i in range(0, n) && i != q,
                   j in range(0, n) && j != q && j != m

      Since condition 1) and 2) are respectively proved, lemma 2 is proved.

  For each iteration, sum(host_next) == sum(host) - 2 - 2
  Apply Lemma 1. host[m] <= sum(<host[i]>, i in range(0, n) && i != m),
  aka. 2*host[m] <= sum(host) is always true.

  Since sum(host) is monotonically decreasing, and 2*host[m] <= sum(host),
  host[m], aka. the maximum value of host inclines to zero.

  The algorithm converges.

  Q.E.D.
